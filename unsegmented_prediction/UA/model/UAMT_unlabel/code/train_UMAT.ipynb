{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "6367c09d-fe10-466f-a91e-ec3641decfa5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from vnet import VNet\n",
    "# from networks.vnet import VNet\n",
    "from dataloaders import utils\n",
    "from utils import ramps, losses\n",
    "#from dataloaders.CTMSpine_sitk import CTMSpine, CTMSpine_unseg, RandomScale, RandomNoise, RandomCrop, CenterCrop, RandomRot, RandomFlip, ToTensor, TransformConsistantOperator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "6e787cbe-47d2-483b-96a7-7a672250b2ef"
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# # 说明:\n",
    "# 此处的label均是onehot,最后一个通道是类别通道\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "# from glob import glob\n",
    "from torch.utils.data import Dataset\n",
    "import h5py\n",
    "import itertools\n",
    "from torch.utils.data.sampler import Sampler\n",
    "# import cv2\n",
    "from skimage import transform\n",
    "\n",
    "class CTMSpine(Dataset):\n",
    "    \"\"\" CTM Spine Dataset \"\"\"\n",
    "    def __init__(self, base_dir=None, split='train', num=None, transform=None, filename=\"mri_norm2.h5\"):\n",
    "        self._base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        self.filename = filename\n",
    "        self.sample_list = []\n",
    "        if split=='train':\n",
    "            with open(self._base_dir+'/../train.list', 'r') as f:\n",
    "                self.image_list = f.readlines()\n",
    "        elif split == 'test':\n",
    "            with open(self._base_dir+'/../test.list', 'r') as f:\n",
    "                self.image_list = f.readlines()\n",
    "        self.image_list = [item.replace('\\n','') for item in self.image_list]\n",
    "        if num is not None:\n",
    "            self.image_list = self.image_list[:num]\n",
    "        print(\"total {} samples\".format(len(self.image_list)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_list[idx]\n",
    "#         image = sitk.ReadImage(self._base_dir+\"/\"+image_name+\"/image.nii.gz\")\n",
    "#         label = sitk.ReadImage(self._base_dir+\"/\"+image_name+\"/label_onehot.nii.gz\")\n",
    "\n",
    "        h5f = h5py.File(self._base_dir+\"/\"+image_name+\"/\"+self.filename, 'r')\n",
    "        image = h5f['image'][:]\n",
    "        label = h5f['label'][:]\n",
    "        label = np.argmax(label,axis=-1)\n",
    "        sample = {'image': image, 'label': label}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "class CTMSpine_unseg(Dataset):\n",
    "    \"\"\" LA Dataset \"\"\"\n",
    "    def __init__(self, base_dir=None, num=None, transform=None, filename=\"center_cut.h5\"):\n",
    "        self._base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        self.filename = filename\n",
    "        self.sample_list = []\n",
    "        print(self._base_dir+'/../../train_unseg_centercut.list')\n",
    "        with open(self._base_dir+'/../../train_unseg.list', 'r') as f:\n",
    "            self.image_list = f.readlines()\n",
    "\n",
    "        self.image_list = [item.replace('\\n','') for item in self.image_list]\n",
    "        if num is not None:\n",
    "            self.image_list = self.image_list[:num]\n",
    "        print(\"total {} samples\".format(len(self.image_list)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #print(self.image_list)\n",
    "        #print(\"check: \",idx,len(self.image_list))\n",
    "        image_name = self.image_list[idx]\n",
    "        h5f = h5py.File(self._base_dir+\"/\"+image_name+\"/\"+self.filename, 'r')\n",
    "        image = h5f['image'][:]\n",
    "        sample = {'image': image,'label':None}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "class CenterCrop(object):\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # pad the sample if necessary\n",
    "        if image.shape[0] <= self.output_size[0] or image.shape[1] <= self.output_size[1] or image.shape[2] <= \\\n",
    "                self.output_size[2]:\n",
    "            pw = max((self.output_size[0] - image.shape[0]) // 2 + 3, 0)\n",
    "            ph = max((self.output_size[1] - image.shape[1]) // 2 + 3, 0)\n",
    "            pd = max((self.output_size[2] - image.shape[2]) // 2 + 3, 0)\n",
    "            image = np.pad(image, [(pw, pw), (ph, ph), (pd, pd)], mode='constant', constant_values=0)\n",
    "            if label is not None:\n",
    "                import pdb\n",
    "                pdb.set_trace()\n",
    "                label = np.pad(label, [(pw, pw), (ph, ph), (pd, pd)], mode='constant', constant_values=0)\n",
    "\n",
    "        (w, h, d) = image.shape\n",
    "\n",
    "        w1 = int(round((w - self.output_size[0]) / 2.))\n",
    "        h1 = int(round((h - self.output_size[1]) / 2.))\n",
    "        d1 = int(round((d - self.output_size[2]) / 2.))\n",
    "\n",
    "        image = image[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
    "        if label:\n",
    "            label = label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "class RandomCrop(object):\n",
    "    \"\"\"\n",
    "    Crop randomly the image in a sample\n",
    "    Args:\n",
    "    output_size (int): Desired output size\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "\n",
    "        # pad the sample if necessary\n",
    "        if image.shape[0] <= self.output_size[0] or image.shape[1] <= self.output_size[1] or image.shape[2] <= \\\n",
    "                self.output_size[2]:\n",
    "            pw = max((self.output_size[0] - image.shape[0]) // 2 + 3, 0)\n",
    "            ph = max((self.output_size[1] - image.shape[1]) // 2 + 3, 0)\n",
    "            pd = max((self.output_size[2] - image.shape[2]) // 2 + 3, 0)\n",
    "            image = np.pad(image, [(pw, pw), (ph, ph), (pd, pd)], mode='constant', constant_values=0)\n",
    "            if label is not None:\n",
    "                label = np.pad(label, [(pw, pw), (ph, ph), (pd, pd)], mode='constant', constant_values=0)\n",
    "\n",
    "        (w, h, d) = image.shape\n",
    "        # if np.random.uniform() > 0.33:\n",
    "        #     w1 = np.random.randint((w - self.output_size[0])//4, 3*(w - self.output_size[0])//4)\n",
    "        #     h1 = np.random.randint((h - self.output_size[1])//4, 3*(h - self.output_size[1])//4)\n",
    "        # else:\n",
    "        w1 = np.random.randint(0, w - self.output_size[0])\n",
    "        h1 = np.random.randint(0, h - self.output_size[1])\n",
    "        d1 = np.random.randint(0, d - self.output_size[2])\n",
    "\n",
    "        image = image[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
    "        if label is not None:\n",
    "            label = label[w1:w1 + self.output_size[0], h1:h1 + self.output_size[1], d1:d1 + self.output_size[2]]\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "# +\n",
    "import SimpleITK as sitk\n",
    "def resample_image3D(\n",
    "    image3D,\n",
    "    spacing=[0.3,0.3,3],\n",
    "    ratio=1.0,\n",
    "    method='Linear',):\n",
    "    \"\"\"做插值\"\"\"\n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    import pdb\n",
    "    pdb.set_trace()\n",
    "    if method == 'Linear':\n",
    "        resample.SetInterpolator(sitk.sitkLinear)\n",
    "    elif method == 'Nearest':\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "    resample.SetOutputDirection( (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) )\n",
    "    resample.SetOutputOrigin((0,0,0))\n",
    "    resample.SetOutputSpacing( (np.array(spacing)*ratio).tolist() )\n",
    "    \n",
    "    newsize = np.round(np.array(image3D.shape)*ratio).astype('int').tolist() \n",
    "    resample.SetSize(newsize)\n",
    "    # resample.SetDefaultPixelValue(0)\n",
    "    print(\"image3D.shape:\",image3D.shape)\n",
    "    image3D = sitk.GetImageFromArray(image3D)\n",
    "    image3D.SetSpacing(spacing)\n",
    "    image3D.SetDirection( (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) )\n",
    "    image3D.SetOrigin((0,0,0))\n",
    "    \n",
    "    newimage = resample.Execute(image3D)\n",
    "    newimage = sitk.GetArrayFromImage(newimage)\n",
    "#     print(\"newimage.shape:\",newimage.shape)\n",
    "    return newimage\n",
    "\n",
    "# def resample_image(image, spacing, ratio, is_label=False):\n",
    "#     # image: 3D image, format: narray\n",
    "#     out_spacing = (np.array(spacing)*ratio).tolist()\n",
    "#     out_size = np.round(np.array(image.shape)*ratio).astype('int').tolist() \n",
    "#     image = sitk.GetImageFromArray(image)\n",
    "#     image.SetSpacing(spacing)\n",
    "#     image.SetDirection( (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) )\n",
    "#     image.SetOrigin((0,0,0))\n",
    "\n",
    "#     resample = sitk.ResampleImageFilter()\n",
    "#     resample.SetOutputSpacing(out_spacing)\n",
    "#     resample.SetSize(out_size)\n",
    "#     resample.SetOutputDirection( (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) )\n",
    "#     resample.SetOutputOrigin( (0,0,0) )\n",
    "#     resample.SetTransform(sitk.Transform())\n",
    "#     resample.SetDefaultPixelValue(2)\n",
    "\n",
    "#     if is_label:\n",
    "#         resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "#     else:\n",
    "#         resample.SetInterpolator(sitk.sitkBSpline)\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    \n",
    "#     out_image = resample.Execute(image) \n",
    "#     out_image = sitk.GetArrayFromImage(out_image)\n",
    "#     return resample.Execute(image) \n",
    "\n",
    "# def resample_image(image, spacing, ratio, is_label=False):\n",
    "#     # image: 3D image, format: narray\n",
    "#     out_spacing = (np.array(spacing)*ratio).tolist()\n",
    "#     out_size = np.round(np.array(image.shape)*ratio).astype('int').tolist() \n",
    "#     image = sitk.GetImageFromArray(image)\n",
    "#     image.SetSpacing(spacing)\n",
    "#     image.SetDirection( (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) )\n",
    "#     image.SetOrigin((0,0,0))\n",
    "\n",
    "#     resample = sitk.ResampleImageFilter()\n",
    "#     resample.SetOutputSpacing(out_spacing)\n",
    "#     resample.SetSize(out_size)\n",
    "#     resample.SetOutputDirection( (1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0) )\n",
    "#     resample.SetOutputOrigin( (0,0,0) )\n",
    "#     resample.SetTransform(sitk.Transform())\n",
    "#     resample.SetDefaultPixelValue(2)\n",
    "\n",
    "#     if is_label:\n",
    "#         resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "#     else:\n",
    "#         resample.SetInterpolator(sitk.sitkBSpline)\n",
    "#     import pdb\n",
    "#     pdb.set_trace()\n",
    "    \n",
    "#     out_image = resample.Execute(image) \n",
    "#     out_image = sitk.GetArrayFromImage(out_image)\n",
    "#     return resample.Execute(image) \n",
    "\n",
    "# def resample_image(image, label, ratio):\n",
    "#     sitkImage = sitk.GetImageFromArray(image, isVector=False)\n",
    "#     sitklabel = sitk.GetImageFromArray(label, isVector=False)\n",
    "\n",
    "#     itemindex = np.where(label > 0)\n",
    "#     randTrans = (0,np.random.randint(-np.min(itemindex[1])/2,(image.shape[1]-np.max(itemindex[1]))/2),np.random.randint(-np.min(itemindex[0])/2,(image.shape[0]-np.max(itemindex[0]))/2))\n",
    "#     translation = sitk.TranslationTransform(3, randTrans)\n",
    "\n",
    "#     resampler = sitk.ResampleImageFilter()\n",
    "#     resampler.SetReferenceImage(sitkImage)\n",
    "#     resampler.SetInterpolator(sitk.sitkLinear)#sitk.sitkBSpline\n",
    "#     resampler.SetDefaultPixelValue(0)\n",
    "#     resampler.SetTransform(translation)\n",
    "\n",
    "#     outimgsitk = resampler.Execute(sitkImage)\n",
    "    \n",
    "#     resampler.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "#     outlabsitk = resampler.Execute(sitklabel)\n",
    "\n",
    "#     outimg = sitk.GetArrayFromImage(outimgsitk)\n",
    "#     outimg = outimg.astype(dtype=float)\n",
    "\n",
    "#     outlbl = sitk.GetArrayFromImage(outlabsitk) > 0\n",
    "#     outlbl = outlbl.astype(dtype=float)\n",
    "\n",
    "#     return outimg, outlbl \n",
    "\n",
    "def resample_image_sitk(image_sitk, label_sitk=None, newspacing=None, out_size=None): \n",
    "    resample = sitk.ResampleImageFilter()\n",
    "    resample.SetOutputDirection(image_sitk.GetDirection())\n",
    "    resample.SetOutputOrigin(image_sitk.GetOrigin())\n",
    "    resample.SetOutputSpacing(newspacing)\n",
    "    \n",
    "    if not out_size:\n",
    "        out_size = np.round(np.array(image_sitk.GetSize())*np.abs(image_sitk.GetSpacing())/np.array(newspacing)).astype('int').tolist()\n",
    "\n",
    "    resample.SetSize(out_size)\n",
    "    # resample.SetDefaultPixelValue(0)\n",
    "    \n",
    "    resample.SetInterpolator(sitk.sitkLinear)\n",
    "    out_image = resample.Execute(image_sitk)\n",
    "    out_image = sitk.GetArrayFromImage(out_image).transpose((2,1,0)).astype(dtype=float)\n",
    "    if label_sitk is None:\n",
    "        return out_image,None\n",
    "    else:\n",
    "        resample.SetInterpolator(sitk.sitkNearestNeighbor)\n",
    "        out_label = resample.Execute(label_sitk)\n",
    "        out_label = sitk.GetArrayFromImage(out_label).transpose((2,1,0,3)).astype(dtype=float)\n",
    "        return out_image, out_label\n",
    "\n",
    "\n",
    "\n",
    "# -\n",
    "\n",
    "class RandomScale(object):\n",
    "    \"\"\"\n",
    "    Scale randomly the image within the scaling ratio of 0.8-1.2\n",
    "    Args:\n",
    "    ratio_low, ratio_high (float): Desired ratio range of random scale \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratio_low, ratio_high):\n",
    "        self.ratio_low = ratio_low\n",
    "        self.ratio_high = ratio_high\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        print(\"type: \",type(image))\n",
    "        \n",
    "        # rescale\n",
    "        ratio = np.random.uniform(self.ratio_low, self.ratio_high)\n",
    "#         image = transform.rescale(image,ratio,order=1,anti_aliasing=True,preserve_range=True,multichannel=False) \n",
    "#         image = resample_image(image, spacing=[0.3, 0.3, 3.0], ratio=ratio, is_label=False)\n",
    "        image,label = resample_image_sitk(image, label, [1.0, 1.0, 1.0])\n",
    "        \n",
    "        assert np.unique(label).tolist() == [0,1,2], \"np.unique(label):\"+str(np.unique(label).tolist())\n",
    "        if label is not None:\n",
    "            image,label = resample_image_sitk(image, label, [1.0, 1.0, 1.0])\n",
    "        else:\n",
    "            image = resample_image_sitk(image, None, [1.0, 1.0, 1.0], None)\n",
    "#             label = transform.rescale(label,ratio,order=0,anti_aliasing=True,preserve_range=True,multichannel=False)\n",
    "            #label = resample_image3D(label,spacing=[0.3,0.3,3],ratio=ratio,method='Nearest')\n",
    "#             label = resample_image(image, spacing=[0.3, 0.3, 3.0], ratio=ratio, is_label=True)\n",
    "#             label = np.argmax(label,axis=-1)\n",
    "#         assert np.unique(label).tolist() == [0,1,2], \"np.unique(rescaled label):\"+str(np.unique(label).tolist())\n",
    "#         print(\"image.shape\",image.shape,\n",
    "#               \"label.shape\",label.shape,\n",
    "#               \"ratio,dsize:\",ratio,dsize,\n",
    "#               \"np.unique(label):\",np.unique(label),\n",
    "#              )\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "# +\n",
    "class TransformConsistantOperator():\n",
    "    \"\"\"\n",
    "    Crop randomly flip the dataset in a sample\n",
    "    Args:\n",
    "    output_size (int): Desired output size\n",
    "    \"\"\"\n",
    "    def __init__(self, k=None, axis=None):\n",
    "        if k is not None:\n",
    "            self.k = k\n",
    "        else:\n",
    "            self.k = np.random.randint(0, 4)\n",
    "        if axis is not None:\n",
    "            self.axis = axis\n",
    "        else:\n",
    "            self.axis = np.random.randint(0, 2)\n",
    "            \n",
    "    def transform(self, image):\n",
    "        \"\"\"image could be image or mask\"\"\"\n",
    "        image = image.permute(2,3,4,0,1)\n",
    "        image = torch.rot90(image, self.k)#np.rot90(image, self.k)\n",
    "        image = torch.flip(image, dims=[self.axis])#np.flip(image, axis=self.axis).copy()\n",
    "        image = image.permute(3,4,0,1,2)\n",
    "\n",
    "#         image = image.permute(2,3,4,0,1).cpu()\n",
    "#         image = np.rot90(image, self.k)\n",
    "#         image = np.flip(image, axis=self.axis).copy()\n",
    "#         image = torch.from_numpy( image.transpose((3,4,0,1,2)).copy() )\n",
    "        return image\n",
    "    \n",
    "    def inv_transform(self, image):\n",
    "        \"\"\"image could be image or mask\"\"\"\n",
    "        image = image.permute(2,3,4,0,1)\n",
    "        image = torch.flip(image, dims=[self.axis])\n",
    "        image = torch.rot90(image, -self.k)\n",
    "        image = image.permute(3,4,0,1,2)\n",
    "\n",
    "#         image = image.permute(2,3,4,0,1).cpu()\n",
    "#         import pdb\n",
    "#         pdb.set_trace()\n",
    "#         image = np.flip(image, axis=self.axis).copy()\n",
    "#         image = np.rot90(image, -self.k)\n",
    "#         image = torch.from_numpy( image.transpose((3,4,0,1,2)).copy() )\n",
    "\n",
    "        return image\n",
    "\n",
    "\n",
    "# -\n",
    "\n",
    "class RandomRot(object):\n",
    "    \"\"\"\n",
    "    Randomly rotate the dataset in a sample\n",
    "    Args:\n",
    "    output_size (int): Desired output size\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        k = np.random.randint(0, 4)\n",
    "        image = np.rot90(image, k)\n",
    "        if label is not None:\n",
    "            label = np.rot90(label, k)\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"\n",
    "    Randomly flip the dataset in a sample\n",
    "    Args:\n",
    "    output_size (int): Desired output size\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        \n",
    "        flip = random.sample([True,False], 1)\n",
    "        if flip:\n",
    "            axis = np.random.randint(0, 2)\n",
    "            image = np.flip(image, axis=axis).copy()\n",
    "        if label is not None:\n",
    "            if flip:\n",
    "                label = np.flip(label, axis=axis).copy()\n",
    "\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "class RandomNoise(object):\n",
    "    def __init__(self, mu=0, sigma=0.1):\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        noise = np.clip(self.sigma * np.random.randn(image.shape[0], image.shape[1], image.shape[2]), -2*self.sigma, 2*self.sigma)\n",
    "        noise = noise + self.mu\n",
    "        image = image + noise\n",
    "        return {'image': image, 'label': label}\n",
    "\n",
    "\n",
    "class CreateOnehotLabel(object):\n",
    "    def __init__(self, num_classes):\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        onehot_label = np.zeros((self.num_classes, label.shape[0], label.shape[1], label.shape[2]), dtype=np.float32)\n",
    "        for i in range(self.num_classes):\n",
    "            onehot_label[i, :, :, :] = (label == i).astype(np.float32)\n",
    "        return {'image': image, 'label': label,'onehot_label':onehot_label}\n",
    "\n",
    "\n",
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['image']\n",
    "        image = image.reshape(1, image.shape[0], image.shape[1], image.shape[2]).astype(np.float32)\n",
    "        label = sample['label']\n",
    "        \n",
    "        if label is not None:\n",
    "            if 'onehot_label' in sample:\n",
    "                return {'image': torch.from_numpy(image), 'label': torch.from_numpy(sample['label']).long(),\n",
    "                        'onehot_label': torch.from_numpy(sample['onehot_label']).long()}\n",
    "            else:\n",
    "                return {'image': torch.from_numpy(image), 'label': torch.from_numpy(sample['label']).long()}\n",
    "        else:\n",
    "            if 'onehot_label' in sample:\n",
    "                return {'image': torch.from_numpy(image),\n",
    "                        'onehot_label': torch.from_numpy(sample['onehot_label']).long()}\n",
    "            else:\n",
    "                return {'image': torch.from_numpy(image)}\n",
    "\n",
    "\n",
    "class TwoStreamBatchSampler(Sampler):\n",
    "    \"\"\"Iterate two sets of indices\n",
    "\n",
    "    An 'epoch' is one iteration through the primary indices.\n",
    "    During the epoch, the secondary indices are iterated through\n",
    "    as many times as needed.\n",
    "    \"\"\"\n",
    "    def __init__(self, primary_indices, secondary_indices, batch_size, secondary_batch_size):\n",
    "        self.primary_indices = primary_indices\n",
    "        self.secondary_indices = secondary_indices\n",
    "        self.secondary_batch_size = secondary_batch_size\n",
    "        self.primary_batch_size = batch_size - secondary_batch_size\n",
    "\n",
    "        assert len(self.primary_indices) >= self.primary_batch_size > 0\n",
    "        assert len(self.secondary_indices) >= self.secondary_batch_size > 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        primary_iter = iterate_once(self.primary_indices)\n",
    "        secondary_iter = iterate_eternally(self.secondary_indices)\n",
    "        return (\n",
    "            primary_batch + secondary_batch\n",
    "            for (primary_batch, secondary_batch)\n",
    "            in zip(grouper(primary_iter, self.primary_batch_size),\n",
    "                    grouper(secondary_iter, self.secondary_batch_size))\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.primary_indices) // self.primary_batch_size\n",
    "\n",
    "def iterate_once(iterable):\n",
    "    return np.random.permutation(iterable)\n",
    "\n",
    "\n",
    "def iterate_eternally(indices):\n",
    "    def infinite_shuffles():\n",
    "        while True:\n",
    "            yield np.random.permutation(indices)\n",
    "    return itertools.chain.from_iterable(infinite_shuffles())\n",
    "\n",
    "\n",
    "def grouper(iterable, n):\n",
    "    \"Collect data into fixed-length chunks or blocks\"\n",
    "    # grouper('ABCDEFG', 3) --> ABC DEF\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip(*args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "uuid": "f5eab87f-be9a-4f5f-8e91-b1d20a3cabfc"
   },
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root_path_labeled', type=str, default='../../data/gz_dataset/segmented')\n",
    "parser.add_argument('--root_path_unlabeled', type=str, default='../../data/gz_dataset/unsegmented/success')\n",
    "parser.add_argument('--exp', type=str,  default='UAMT_unlabel', help='model_name')\n",
    "parser.add_argument('--max_iterations', type=int,  default=6000, help='maximum epoch number to train')\n",
    "parser.add_argument('--batch_size', type=int, default=2, help='batch_size per gpu')\n",
    "parser.add_argument('--labeled_bs', type=int, default=1, help='labeled_batch_size per gpu')\n",
    "parser.add_argument('--base_lr', type=float,  default=0.01, help='maximum epoch number to train')\n",
    "parser.add_argument('--deterministic', type=int,  default=1, help='whether use deterministic training')\n",
    "parser.add_argument('--seed', type=int,  default=1337, help='random seed')\n",
    "parser.add_argument('--gpu', type=str,  default='0', help='GPU to use')\n",
    "### costs\n",
    "parser.add_argument('--ema_decay', type=float,  default=0.99, help='ema_decay')\n",
    "parser.add_argument('--consistency_type', type=str,  default=\"mse\", help='consistency_type')\n",
    "parser.add_argument('--consistency', type=float,  default=0.1, help='consistency')\n",
    "parser.add_argument('--consistency_rampup', type=float,  default=40.0, help='consistency_rampup')\n",
    "args = parser.parse_args(args=[])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "uuid": "cd299cce-2899-43ba-b702-a95145d5c879"
   },
   "outputs": [],
   "source": [
    "labeled_train_data_path = args.root_path_labeled\n",
    "unlabeled_train_data_path = args.root_path_unlabeled\n",
    "snapshot_path = \"../model/\" + args.exp + \"/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "b439c8f0-d716-4d71-ab72-6126ffed822d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "torch.cuda.empty_cache()\n",
    "batch_size = args.batch_size * len(args.gpu.split(','))\n",
    "print(batch_size)\n",
    "max_iterations = args.max_iterations\n",
    "base_lr = args.base_lr\n",
    "labeled_bs = args.labeled_bs * len(args.gpu.split(','))\n",
    "print(labeled_bs)\n",
    "\n",
    "if args.deterministic:\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "num_classes = 4\n",
    "patch_size = (128, 128, 64)#(128, 128, 64)\n",
    "cls_weights = [1,5,5,15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "80458b50-c84b-4374-b8ff-8790bb4c4f34"
   },
   "outputs": [],
   "source": [
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    return args.consistency * ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "28a2808d-603f-47b9-b7e3-73348cf35fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(base_lr=0.01, batch_size=4, consistency=0.1, consistency_rampup=40.0, consistency_type='mse', deterministic=1, ema_decay=0.99, exp='UAMT_unlabel', gpu='0', labeled_bs=2, max_iterations=6000, root_path_labeled='../../data/gz_dataset/segmented', root_path_unlabeled='../../data/gz_dataset/unsegmented/success', seed=1337)\n",
      "total 32 samples\n",
      "../../data/gz_dataset/unsegmented/success/../../train_unseg_centercut.list\n",
      "total 42 samples\n",
      "21 itertations per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                         | 0/376 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epoch: 376\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.550\n",
      "dice score (1-dice_loss): 0.202\n",
      "dice score (1-dice_loss): 0.138\n",
      "dice score (1-dice_loss): 0.111\n",
      "iteration 1 : loss : 2.204197, loss_seg : 1.410044, loss_seg_dice : 2.998127, consistency_loss : 0.000111, cons_dist: 0.165281, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.569\n",
      "dice score (1-dice_loss): 0.215\n",
      "dice score (1-dice_loss): 0.160\n",
      "dice score (1-dice_loss): 0.095\n",
      "iteration 2 : loss : 2.148988, loss_seg : 1.336917, loss_seg_dice : 2.960974, consistency_loss : 0.000042, cons_dist: 0.062557, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.486\n",
      "dice score (1-dice_loss): 0.003\n",
      "dice score (1-dice_loss): 0.199\n",
      "dice score (1-dice_loss): 0.005\n",
      "iteration 3 : loss : 2.342353, loss_seg : 1.377737, loss_seg_dice : 3.306852, consistency_loss : 0.000058, cons_dist: 0.086155, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.600\n",
      "dice score (1-dice_loss): 0.186\n",
      "dice score (1-dice_loss): 0.151\n",
      "dice score (1-dice_loss): 0.150\n",
      "iteration 4 : loss : 2.081464, loss_seg : 1.249177, loss_seg_dice : 2.913618, consistency_loss : 0.000067, cons_dist: 0.099580, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.570\n",
      "dice score (1-dice_loss): 0.280\n",
      "dice score (1-dice_loss): 0.170\n",
      "dice score (1-dice_loss): 0.118\n",
      "iteration 5 : loss : 2.093072, loss_seg : 1.324432, loss_seg_dice : 2.861637, consistency_loss : 0.000038, cons_dist: 0.056699, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.662\n",
      "dice score (1-dice_loss): 0.127\n",
      "dice score (1-dice_loss): 0.350\n",
      "dice score (1-dice_loss): 0.071\n",
      "iteration 6 : loss : 1.936480, loss_seg : 1.082831, loss_seg_dice : 2.790092, consistency_loss : 0.000019, cons_dist: 0.027615, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.705\n",
      "dice score (1-dice_loss): 0.212\n",
      "dice score (1-dice_loss): 0.246\n",
      "dice score (1-dice_loss): 0.068\n",
      "iteration 7 : loss : 1.913128, loss_seg : 1.057260, loss_seg_dice : 2.768898, consistency_loss : 0.000049, cons_dist: 0.072836, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.603\n",
      "dice score (1-dice_loss): 0.178\n",
      "dice score (1-dice_loss): 0.359\n",
      "dice score (1-dice_loss): 0.051\n",
      "iteration 8 : loss : 2.034842, loss_seg : 1.260602, loss_seg_dice : 2.808988, consistency_loss : 0.000046, cons_dist: 0.068686, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.709\n",
      "dice score (1-dice_loss): 0.041\n",
      "dice score (1-dice_loss): 0.299\n",
      "dice score (1-dice_loss): 0.070\n",
      "iteration 9 : loss : 1.964561, loss_seg : 1.046716, loss_seg_dice : 2.882354, consistency_loss : 0.000026, cons_dist: 0.038667, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.732\n",
      "dice score (1-dice_loss): 0.121\n",
      "dice score (1-dice_loss): 0.400\n",
      "dice score (1-dice_loss): 0.088\n",
      "iteration 10 : loss : 1.853935, loss_seg : 1.048193, loss_seg_dice : 2.659597, consistency_loss : 0.000041, cons_dist: 0.060436, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.675\n",
      "dice score (1-dice_loss): 0.083\n",
      "dice score (1-dice_loss): 0.393\n",
      "dice score (1-dice_loss): 0.074\n",
      "iteration 11 : loss : 1.928408, loss_seg : 1.081112, loss_seg_dice : 2.775649, consistency_loss : 0.000028, cons_dist: 0.040936, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.734\n",
      "dice score (1-dice_loss): 0.156\n",
      "dice score (1-dice_loss): 0.417\n",
      "dice score (1-dice_loss): 0.058\n",
      "iteration 12 : loss : 1.786457, loss_seg : 0.937522, loss_seg_dice : 2.635356, consistency_loss : 0.000018, cons_dist: 0.026961, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.809\n",
      "dice score (1-dice_loss): 0.200\n",
      "dice score (1-dice_loss): 0.272\n",
      "dice score (1-dice_loss): 0.028\n",
      "iteration 13 : loss : 1.852190, loss_seg : 1.013184, loss_seg_dice : 2.691076, consistency_loss : 0.000060, cons_dist: 0.089097, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.721\n",
      "dice score (1-dice_loss): 0.138\n",
      "dice score (1-dice_loss): 0.314\n",
      "dice score (1-dice_loss): 0.089\n",
      "iteration 14 : loss : 1.964482, loss_seg : 1.190335, loss_seg_dice : 2.738571, consistency_loss : 0.000029, cons_dist: 0.042490, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.860\n",
      "dice score (1-dice_loss): 0.128\n",
      "dice score (1-dice_loss): 0.587\n",
      "dice score (1-dice_loss): 0.155\n",
      "iteration 15 : loss : 1.525935, loss_seg : 0.782688, loss_seg_dice : 2.269108, consistency_loss : 0.000037, cons_dist: 0.054181, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.794\n",
      "dice score (1-dice_loss): 0.162\n",
      "dice score (1-dice_loss): 0.429\n",
      "dice score (1-dice_loss): 0.122\n",
      "iteration 16 : loss : 1.700463, loss_seg : 0.908025, loss_seg_dice : 2.492812, consistency_loss : 0.000044, cons_dist: 0.065117, loss_weight: 0.000674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                               | 1/376 [01:08<7:08:39, 68.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.874\n",
      "dice score (1-dice_loss): 0.213\n",
      "dice score (1-dice_loss): 0.466\n",
      "dice score (1-dice_loss): 0.197\n",
      "iteration 17 : loss : 1.663672, loss_seg : 1.077228, loss_seg_dice : 2.250080, consistency_loss : 0.000018, cons_dist: 0.026572, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.858\n",
      "dice score (1-dice_loss): 0.030\n",
      "dice score (1-dice_loss): 0.517\n",
      "dice score (1-dice_loss): 0.101\n",
      "iteration 18 : loss : 1.584368, loss_seg : 0.675163, loss_seg_dice : 2.493521, consistency_loss : 0.000026, cons_dist: 0.038153, loss_weight: 0.000674\n",
      "\n",
      "\n",
      "dice score (1-dice_loss): 0.848\n",
      "dice score (1-dice_loss): 0.148\n",
      "dice score (1-dice_loss): 0.485\n",
      "dice score (1-dice_loss): 0.118\n",
      "iteration 19 : loss : 1.646052, loss_seg : 0.889790, loss_seg_dice : 2.402240, consistency_loss : 0.000037, cons_dist: 0.055449, loss_weight: 0.000674\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    ## make logger file\n",
    "    if not os.path.exists(snapshot_path):\n",
    "        os.makedirs(snapshot_path)\n",
    "    if os.path.exists(snapshot_path + '/code'):\n",
    "        shutil.rmtree(snapshot_path + '/code')\n",
    "    shutil.copytree('.', snapshot_path + '/code', shutil.ignore_patterns(['.git','__pycache__']))\n",
    "\n",
    "    logging.basicConfig(filename=snapshot_path+\"/log.txt\", level=logging.INFO,\n",
    "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info(str(args))\n",
    "\n",
    "    def create_model(ema=False):\n",
    "        # Network definition\n",
    "        net = VNet(n_channels=1, n_classes=num_classes, normalization='batchnorm', has_dropout=True)\n",
    "        model = net.cuda()\n",
    "        if ema:\n",
    "            for param in model.parameters():\n",
    "                param.detach_()\n",
    "        return model\n",
    "\n",
    "    model = create_model()\n",
    "    ema_model = create_model(ema=True)\n",
    "    \n",
    "    #　pytorch 的数据加载到模型的操作顺序（三板斧）：\n",
    "    #    ① 创建一个 Dataset 对象\n",
    "    #    ② 创建一个 DataLoader 对象\n",
    "    #    ③ 循环这个 DataLoader 对象，将img, label加载到模型中进行训练\n",
    "    db_train_labeled = CTMSpine(\n",
    "        base_dir=labeled_train_data_path,\n",
    "        split='train',\n",
    "        transform = transforms.Compose([\n",
    "            #RandomScale(ratio_low=0.8, ratio_high=1.2),\n",
    "            RandomNoise(mu=0, sigma=0.05),\n",
    "            RandomRot(),\n",
    "            RandomFlip(),\n",
    "            RandomCrop(patch_size),\n",
    "            ToTensor(),\n",
    "        ]))\n",
    "    db_train_unlabeled = CTMSpine_unseg(\n",
    "        base_dir=unlabeled_train_data_path,\n",
    "        transform = transforms.Compose([\n",
    "            #RandomScale(ratio_low=0.8, ratio_high=1.2),\n",
    "            RandomNoise(mu=0, sigma=0.05),\n",
    "            RandomRot(),\n",
    "            RandomFlip(),\n",
    "            RandomCrop(patch_size),\n",
    "            ToTensor(),\n",
    "        ]))#因为计算一致性损失时增加了噪声，所以不在此处加噪声\n",
    "#     db_test = LAHeart(base_dir=labeled_train_data_path,\n",
    "#                        split='test',\n",
    "#                        transform = transforms.Compose([\n",
    "#                            CenterCrop(patch_size),\n",
    "#                            ToTensor()\n",
    "#                        ]))\n",
    "    \n",
    "\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(args.seed+worker_id)\n",
    "    # 在linux系统中可以使用多个子进程加载数据，而在windows系统中不能。所以在windows中要将DataLoader中的num_workers设置为0或者采用默认为0的设置。\n",
    "    #trainloader = DataLoader(db_train, batch_sampler=batch_sampler, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "    #trainloader = DataLoader(db_train_labeled, batch_sampler=batch_sampler, num_workers=0, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "    \n",
    "    labeled_trainloader = DataLoader(db_train_labeled, batch_size=labeled_bs, shuffle=True, num_workers=2, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "    unlabeled_trainloader = DataLoader(db_train_unlabeled, batch_size=batch_size-labeled_bs, shuffle=True, num_workers=2, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    model.train()\n",
    "    ema_model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "    if args.consistency_type == 'mse':\n",
    "        consistency_criterion = losses.softmax_mse_loss\n",
    "    elif args.consistency_type == 'kl':\n",
    "        consistency_criterion = losses.softmax_kl_loss\n",
    "    else:\n",
    "        assert False, args.consistency_type\n",
    "\n",
    "    writer = SummaryWriter(snapshot_path+'/log')\n",
    "    logging.info(\"{} itertations per epoch\".format(len(unlabeled_trainloader)))\n",
    "\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations//len(labeled_trainloader)+1\n",
    "    print(\"max_epoch:\",max_epoch)\n",
    "    lr_ = base_lr\n",
    "    model.train()\n",
    "    for epoch_num in tqdm(range(max_epoch), ncols=70):\n",
    "        time1 = time.time()\n",
    "        for i_batch, (sampled_batch_labeled, sampled_batch_unlabeled) in enumerate( zip(labeled_trainloader, unlabeled_trainloader) ):\n",
    "            time2 = time.time()\n",
    "            # print('fetch data cost {}'.format(time2-time1))\n",
    "            labeled_volume_batch, label_batch = sampled_batch_labeled['image'], sampled_batch_labeled['label']\n",
    "            unlabeled_volume_batch = sampled_batch_unlabeled['image']\n",
    "            unlabeled_volume_batch = torch.cat((labeled_volume_batch,unlabeled_volume_batch),dim=0)\n",
    "            labeled_volume_batch, label_batch = labeled_volume_batch.cuda(), label_batch.cuda()\n",
    "            unlabeled_volume_batch = unlabeled_volume_batch.cuda()\n",
    "\n",
    "            noise = torch.clamp(torch.randn_like(unlabeled_volume_batch) * 0.05, -0.05, 0.05)\n",
    "            ema_inputs = unlabeled_volume_batch + noise\n",
    "            \n",
    "            outputs = model(labeled_volume_batch)\n",
    "            unlabeled_outputs = model(unlabeled_volume_batch)\n",
    "            with torch.no_grad():\n",
    "                ema_output = ema_model(ema_inputs)\n",
    "            T = 8\n",
    "            volume_batch_r = unlabeled_volume_batch.repeat(2, 1, 1, 1, 1)\n",
    "            stride = volume_batch_r.shape[0] // 2\n",
    "            preds = torch.zeros([stride * T, num_classes, patch_size[0], patch_size[1], patch_size[2]]).cuda()\n",
    "            for i in range(T//2):\n",
    "                TCO = TransformConsistantOperator(k=i, axis=np.random.randint(0, 2))\n",
    "                ema_inputs = volume_batch_r + torch.clamp(torch.randn_like(volume_batch_r) * 0.1, -0.2, 0.2)\n",
    "                with torch.no_grad():\n",
    "                    ema_inputs = TCO.transform(ema_inputs).cuda()\n",
    "                    pred = ema_model(ema_inputs)\n",
    "                    pred = TCO.inv_transform(pred).cuda()\n",
    "                    preds[2 * stride * i:2 * stride * (i + 1)] = pred\n",
    "            preds = F.softmax(preds, dim=1)\n",
    "            #preds = preds.reshape(T, stride, 2, 112, 112, 80)\n",
    "            preds = preds.reshape(T, stride, num_classes, patch_size[0], patch_size[1], patch_size[2])\n",
    "            preds = torch.mean(preds, dim=0)  #(batch, 2, 112,112,80)\n",
    "            uncertainty = -1.0*torch.sum(preds*torch.log(preds + 1e-6), dim=1, keepdim=True)\n",
    "\n",
    "            ## calculate the loss(only for labeled samples)\n",
    "            loss_seg = F.cross_entropy( outputs, label_batch, weight=torch.tensor(cls_weights,dtype=torch.float32).cuda() )\n",
    "            outputs_soft = F.softmax(outputs, dim=1)\n",
    "            loss_seg_dice = 0\n",
    "            print('\\n')\n",
    "            for i in range(num_classes):\n",
    "                loss_mid = losses.dice_loss(outputs_soft[:, i, :, :, :], label_batch == i )\n",
    "                loss_seg_dice += loss_mid\n",
    "                print('dice score (1-dice_loss): {:.3f}'.format(1-loss_mid))\n",
    "\n",
    "#             print('dicetotal:{:.3f}'.format( loss_seg_dice))\n",
    "            #loss_seg_dice = losses.dice_loss(outputs_soft[:labeled_bs, 1, :, :, :], label_batch[:labeled_bs] == 1)\n",
    "            supervised_loss = 0.5*(loss_seg+loss_seg_dice)\n",
    "            \n",
    "            # only for unlabeled samples\n",
    "            consistency_weight = get_current_consistency_weight(iter_num//150)\n",
    "            consistency_dist = consistency_criterion(unlabeled_outputs, ema_output) #(batch, num_classes, 112,112,80)\n",
    "            threshold = (0.75+0.25*ramps.sigmoid_rampup(iter_num, max_iterations))*np.sqrt(3)#N分类问题的最大不确定度是sqrt(N)\n",
    "            mask = (uncertainty<threshold).float()\n",
    "#             print(\"consistency_dist:\",consistency_dist.item())\n",
    "            asd = np.prod( list(mask.shape) )\n",
    "            #print(\"mask:\",np.sum(mask.item())/asd )\n",
    "            consistency_dist = torch.sum(mask*consistency_dist)/(2*torch.sum(mask)+1e-16)\n",
    "            consistency_loss = consistency_weight * consistency_dist\n",
    "            loss = supervised_loss + consistency_loss\n",
    "            \n",
    "            \n",
    "            # pytorch模型训练的三板斧\n",
    "            # 一般训练神经网络，总是逃不开optimizer.zero_grad之后是loss（后面有的时候还会写forward，看你网络怎么写了）之后是是net.backward之后是optimizer.step的这个过程\n",
    "            optimizer.zero_grad()#把模型中参数的梯度设为0\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            update_ema_variables(model, ema_model, args.ema_decay, iter_num)\n",
    "\n",
    "            iter_num = iter_num + 1\n",
    "            writer.add_scalar('uncertainty/mean', uncertainty[0,0].mean(), iter_num)\n",
    "            writer.add_scalar('uncertainty/max', uncertainty[0,0].max(), iter_num)\n",
    "            writer.add_scalar('uncertainty/min', uncertainty[0,0].min(), iter_num)\n",
    "            writer.add_scalar('uncertainty/mask_per', torch.sum(mask)/mask.numel(), iter_num)\n",
    "            writer.add_scalar('uncertainty/threshold', threshold, iter_num)\n",
    "            writer.add_scalar('lr', lr_, iter_num)\n",
    "            writer.add_scalar('loss/loss', loss, iter_num)\n",
    "            writer.add_scalar('loss/loss_seg', loss_seg, iter_num)\n",
    "            writer.add_scalar('loss/loss_seg_dice', loss_seg_dice, iter_num)\n",
    "            writer.add_scalar('train/consistency_loss', consistency_loss, iter_num)\n",
    "            writer.add_scalar('train/consistency_weight', consistency_weight, iter_num)\n",
    "            writer.add_scalar('train/consistency_dist', consistency_dist, iter_num)\n",
    "\n",
    "            logging.info('iteration %d : loss : %f, loss_seg : %f, loss_seg_dice : %f, consistency_loss : %f, cons_dist: %f, loss_weight: %f' %\n",
    "                         (iter_num, \n",
    "                          loss.item(), \n",
    "                          loss_seg.item(),\n",
    "                          loss_seg_dice.item(),\n",
    "                          consistency_loss.item(),\n",
    "                          consistency_dist.item(),\n",
    "                          consistency_weight))\n",
    "            if iter_num % 50 == 0:\n",
    "                image = labeled_volume_batch[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)#repeat 3是为了模拟图像的RGB三个通道\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('train/Image', grid_image, iter_num)\n",
    "\n",
    "                # image = outputs_soft[0, 3:4, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                image = torch.max(outputs_soft[0, :, :, :, 20:61:10], 0)[1].permute(2, 0, 1).data.cpu().numpy()\n",
    "                image = utils.decode_seg_map_sequence(image)\n",
    "                grid_image = make_grid(image, 5, normalize=False)\n",
    "                writer.add_image('train/Predicted_label', grid_image, iter_num)\n",
    "\n",
    "                image = label_batch[0, :, :, 20:61:10].permute(2, 0, 1)\n",
    "                grid_image = make_grid(utils.decode_seg_map_sequence(image.data.cpu().numpy()), 5, normalize=False)\n",
    "                writer.add_image('train/Groundtruth_label', grid_image, iter_num)\n",
    "\n",
    "                image = uncertainty[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('train/uncertainty', grid_image, iter_num)\n",
    "\n",
    "                mask2 = (uncertainty > threshold).float()\n",
    "                image = mask2[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('train/mask', grid_image, iter_num)\n",
    "                #####\n",
    "                image = labeled_volume_batch[-1, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('unlabel/Image', grid_image, iter_num)\n",
    "\n",
    "                # image = outputs_soft[-1, 3:4, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                image = torch.max(outputs_soft[-1, :, :, :, 20:61:10], 0)[1].permute(2, 0, 1).data.cpu().numpy()\n",
    "                image = utils.decode_seg_map_sequence(image)\n",
    "                grid_image = make_grid(image, 5, normalize=False)\n",
    "                writer.add_image('unlabel/Predicted_label', grid_image, iter_num)\n",
    "\n",
    "                image = label_batch[-1, :, :, 20:61:10].permute(2, 0, 1)\n",
    "                grid_image = make_grid(utils.decode_seg_map_sequence(image.data.cpu().numpy()), 5, normalize=False)\n",
    "                writer.add_image('unlabel/Groundtruth_label', grid_image, iter_num)\n",
    "\n",
    "            ## change lr\n",
    "            if iter_num % 2500 == 0:\n",
    "                lr_ = base_lr * 0.1 ** (iter_num // 2500)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_\n",
    "            if iter_num % 1000 == 0:\n",
    "                save_mode_path = os.path.join(snapshot_path, 'iter_' + str(iter_num) + '.pth')\n",
    "                torch.save(model.state_dict(), save_mode_path)\n",
    "                logging.info(\"save model to {}\".format(save_mode_path))\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "            time1 = time.time()\n",
    "        if iter_num >= max_iterations:\n",
    "            break\n",
    "    save_mode_path = os.path.join(snapshot_path, 'iter_'+str(max_iterations)+'.pth')\n",
    "    torch.save(model.state_dict(), save_mode_path)\n",
    "    logging.info(\"save model to {}\".format(save_mode_path))\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "162d033f-b19a-4151-af57-cc1c0b6810dd"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
