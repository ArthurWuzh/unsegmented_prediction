{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验记录:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "2020-12-9:\n",
    "标记数据１８１例，无标记数据220例\n",
    "类别权重[1,1,1]\n",
    "\n",
    "(1):\n",
    "测试集：\n",
    "average metric is \n",
    "dice:[0.99775189 0.83457938 0.72158159]\n",
    "jc:  [0.99551699 0.72002847 0.59806289]\n",
    "hd:  [0.38326324 5.88348262 6.03918977]\n",
    "asd: [0.13204016 1.58062675 1.22393472]]\n",
    "测试时，patch和stride取值对指标几乎无影响。\n",
    "\n",
    "\n",
    "(2):\n",
    "dice 类别权重设置为【0,1,1】:\n",
    "average metric is \n",
    "dice: [ 0.99738669  0.81139882  0.73131243]\n",
    "jc:   [ 0.99478955  0.68721904  0.61365896]\n",
    "hd:   [ 0.62857143 13.15592863  4.03040443]\n",
    "asd:  [ 0.19392581  3.15095933  0.9735733 ]]\n",
    "\n",
    "(3):\n",
    "dice 类别权重设置为【0,4,10】:\n",
    "结果不理想。\n",
    "\n",
    "(4):\n",
    "cross_entropy的类别权重设置为【0,4,10】,dice无类别权重如下。\n",
    "与之前的模型结构比对，dura\\SC的dice均有提升，可见cross_entropy的类别加权，对提升dice是有效的。\n",
    "average metric is \n",
    "dice: [0.9980312  0.8560623  0.78079157]\n",
    "jc:   [0.99607137 0.75099396 0.67602391]]\n",
    " \n",
    " \n",
    "(5):\n",
    "patch_size由(112, 112, 32)改为(128, 128, 64):\n",
    "与(4)对比可见，增大patch_size是有效的。\n",
    "\n",
    "average metric is ## patch_size=(112, 112, 32), stride_xy=56, stride_z=16\n",
    "dice: [0.99838061 0.86879789 0.87586517]\n",
    "jc:   [0.9967683  0.77383196 0.78492811]] \n",
    "##若patch_size=(128, 128, 64), stride_xy=64, stride_z=32，则：\n",
    "average metric is \n",
    "dice: [0.99851892 0.8833286  0.85924949]\n",
    "jc:   [0.99704313 0.79440783 0.76164823]\n",
    "\n",
    "\n",
    "(6):\n",
    "为了考察半监督学习对效果的改进程度，以VNet作为基准，除了仅仅采用有标签数据来训练之外，VNet的所有网络结构、超参数、损失函数等等均以半监督学习中的student模型一致。在测试集上的结果如下：\n",
    "average metric is \n",
    "dice: [0.99860859 0.88433452 0.86901513]\n",
    "jc:   [0.99722239 0.7985549  0.77530212]\n",
    "\n",
    "(7):\n",
    "半监督学习，Transform Consistent Operator那一块,添加了随机旋转＋翻转。\n",
    "但是,由于代码原因，只用了一个GPU,之前是４个gpu\n",
    "average metric is:\n",
    "dice: [0.99811286 0.85622213 0.84583206]\n",
    "jc:   [0.99623409 0.7521626  0.74122247]\n",
    "\n",
    "(8):\n",
    "半监督学习部分添加的高斯噪声可能过大，因此在实验（７）基础上做一下更改：\n",
    "１）减小噪声标准差（0.1变为0.05），截断区间由[-0.2,0.2]变为[-0.05,0.05]。\n",
    "２）训练数据的扩增操作，添加一个噪声项，sig=0,mu=0.05。\n",
    "发现，consistency_loss在100次迭代之后就已经降到1e-6量级了。说明无标签数据几乎不起作用。\n",
    "相比（７）有１个百分点的提升,可见１)或2)措施有效。\n",
    "average metric is:\n",
    "dice: [0.99826443 0.86610701 0.85446536]\n",
    "jc:   [0.99653613 0.76670752 0.75200748]\n",
    "\n",
    "(9):\n",
    "为了考核RandomScale, RandomNoise的效用。今天时间有限，因此以VNet为研究对象，在其中添加这两数据扩增操作。结果如下：\n",
    "与（６）相比,指标降低了，可见无效。。。\n",
    "average metric is:\n",
    "dice: [0.99850791 0.87725577 0.83228357]\n",
    "jc:   [0.99702122 0.78541586 0.71834169]\n",
    "\n",
    "(10)：\n",
    "UM-AT模型，标签数据部分增加RandomScale, RandomNoise。\n",
    "相比（8），指标降低，说明：1）可能这两个数据扩增措施至少一个无效或有害；2）这两数据扩张措施有效，但是会使得训练的收敛变慢，迭代6000次还不足以收敛，所以指标变差。\n",
    "average metric is:\n",
    "dice: [0.99796083 0.84502699 0.80342992]\n",
    "jc:   [0.9959313  0.73561608 0.68334987]\n",
    "\n",
    "\n",
    "\n",
    "留意到：\n",
    "Namespace(base_lr=0.01, batch_size=8, consistency=0.1, consistency_rampup=40.0, consistency_type='mse', deterministic=1, ema_decay=0.99, exp='UAMT_unlabel', gpu='4', labeled_bs=2, max_iterations=6000, root_path_labeled='/home/cyagen/tyler/CTM/UA-MT/data/CTM_dataset/Segmented', root_path_unlabeled='/home/cyagen/tyler/CTM/UA-MT/data/CTM_dataset/unSegmented', seed=1337)\n",
    "total 146 samples\n",
    "total 12 samples\n",
    "total 35 samples\n",
    "2 itertations per epoch\n",
    "  0%|                                        | 0/3001 [00:00<?, ?it/s]\n",
    "  \n",
    "显示无标签数据的总数量是12，说明之前的所有半监督学习实验中无标签数据读取错误了。\n",
    "\n",
    "此外：\n",
    "半监督学习的训练：max_epoch = max_iterations//len(unlabeled_trainloader)+1\n",
    "而VNet的训练：max_epoch = max_iterations//len(labeled_trainloader)+1\n",
    "导致前者比后者训练次数少。这是不公平的，所以统一用：\n",
    "max_epoch = max_iterations//len(labeled_trainloader)+1\n",
    "\n",
    "\n",
    "(11)\n",
    "改正无标签数据的错误、max_epoch错误之后，删除所有randomScale、randomNoise，删除TCO, 用4号GPU训练6000个iter。\n",
    "average metric is:\n",
    "[0.99808617 0.8434132  0.81268442]\n",
    "课件dura和SC的dice都很低。可能原因；1）无标签数据是有害的；2）GPU数量少了（之前无论训练半监督学习还是训练VNet,都是采用四个GPU）;3)无监督学习需要更多的迭代次数才能性能收敛？\n",
    "\n",
    "(12)\n",
    "(12-1)\n",
    "为了检验（11）中指标下降，是不是\"1）\"原因，采用1个GPU训练VNet。注意：删除所有randomScale、randomNoise。\n",
    "average metric is:\n",
    "[0.99862201 0.87959961 0.81599167]\n",
    "\n",
    "(12-2)半监督学习的迭代次数改为10000，重新训练。\n",
    "发现模型提前终止训练了：\n",
    "iteration 5004 : loss : 0.514611, loss_seg : 0.146374, loss_seg_dice : 0.882710, consistency_loss : 0.000069, cons_dist: 0.000806, loss_weight: 0.085802\n",
    "100%|#############################| 139/139 [3:36:44<00:00, 93.55s/it]\n",
    "save model to ../model/UAMT_unlabel/iter_10000.pth\n",
    "测试结果如下：\n",
    "average metric is:\n",
    "[0.99842905 0.86174148 0.82296542]\n",
    "5000个iter的测试结果竟然比（11）中的6000个iter的测试结果好。说明5000个或6000个iter已经足够半监督模型收敛了。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "为了检查（10）中的RandomScale, RandomNoise哪一个无效/有害，删除有标签数据和无标签数据中的RandomScale。\n",
    "\n",
    "(13):\n",
    "采用dice_loss1:\n",
    "发现在3000iter提前停止了训练：\n",
    "iteration 3024 : loss : 0.396193, loss_seg : 0.029810, loss_seg_dice : 0.762458, consistency_loss : 0.000059, cons_dist: 0.002059, loss_weight: 0.028650\n",
    "100%|###############################| 84/84 [2:11:12<00:00, 93.71s/it]\n",
    "save model to ../model/UAMT_unlabel/iter_6000.pth\n",
    "\n",
    "与(12-2)相比，准确度下降了。。。\n",
    "average metric is:\n",
    "[0.99835612 0.85874714 0.79975291]\n",
    "\n",
    "(14)\n",
    "恢复采用dice_loss,扩大patch_size = (128, 128, 96)\n",
    "iteration 5004 : loss : 0.419328, loss_seg : 0.104150, loss_seg_dice : 0.734416, consistency_loss : 0.000045, cons_dist: 0.000523, loss_weight: 0.085802\n",
    "100%|############################| 139/139 [5:23:58<00:00, 139.85s/it]\n",
    "save model to ../model/UAMT_unlabel/iter_10000.pth\n",
    "与(12-2)相比，SC指标略降低，而dura有提升。\n",
    "average metric is:\n",
    "[0.99851249 0.86937044 0.80912372]\n",
    "发现一个大问题：之前测试的时候没有设置iter=10000(手动捂脸)！！！！！\n",
    "\n",
    "(15)\n",
    "各种招数都用上：\n",
    "average metric is:\n",
    "[0.99839574 0.85325123 0.76796092]\n",
    "(16):\n",
    "在(15)的基础上，\n",
    "将：\n",
    "mask = (uncertainty<threshold).float()\n",
    "改为：\n",
    "mask = (uncertainty>threshold).float()\n",
    "dice几乎没变化，这正说明了无标签数据没发挥作用。\n",
    "average metric is:\n",
    "[0.99833737 0.85146968 0.77290092]\n",
    "\n",
    "(17)\n",
    "对比实验（6），原始数据preprocessing的时候，不做下采样。无RandomScale、RandomNoise(mu=0, sigma=0.05)。\n",
    "训练VNet,batchsize=2.\n",
    "average metric is:\n",
    "[0.99444295 0.86842974 0.8184397 ]\n",
    "\n",
    "(18)\n",
    "对比（17）,batchsize=8\n",
    "average metric is:\n",
    "[0.99585844 0.90564325 0.8533867 ]\n",
    "\n",
    "(19)\n",
    "对比（18）,randomRot和randomFlip解耦，并且randomFlip也1/2随机执行/不执行。\n",
    "average metric is:\n",
    "[0.99589649 0.90616521 0.85298152]\n",
    "\n",
    "(20)\n",
    "对比（19）。修正了数据预处理的两个错误：\n",
    "1)删除6个错误标注病例（确守其中一个类别，图像和mask的shape不一致）。\n",
    "2）应该用切割前的相熟做归一化。\n",
    "average metric is:\n",
    "[0.9961081  0.91436556 0.87028409]\n",
    "\n",
    "(21)\n",
    "对比（20）。训练数据增加了随机噪声数据扩增。\n",
    "average metric is:\n",
    "[0.9961625  0.91434658 0.88042991]\n",
    "\n",
    "(22)\n",
    "对比（21）。数据预处理时采用SimpleITK将spacing统一为[0.3,0.3,3.0]。训练时候不做randomScale\n",
    "average metric is:\n",
    "[0.99462905 0.91411392 0.85618845]\n",
    "\n",
    "(23)\n",
    "对比（22）。训练时候做randomScale：RandomScale(ratio_low=0.8, ratio_high=1.2)\n",
    "average metric is:\n",
    "[0.9934569  0.89229473 0.86884025\n",
    "\n",
    "(24)\n",
    "对比（23），训练期间label数据扩增中skimage.transform.rescale的order=0(而23是order=1)\n",
    "average metric is:\n",
    "[0.99335168 0.88883402 0.87202118]\n",
    "\n",
    "(25)\n",
    "对比（24），训练期间image/label数据扩增中skimage.transform.rescale的anti_aliasing=True\n",
    "average metric is:\n",
    "[0.99311405 0.88651117 0.86000655]\n",
    "\n",
    "(26)\n",
    "2021-03-05\n",
    "终于修好bug,搞定数据预处理,代码见:\n",
    "CTM_data_processing_V2-sitk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argmax( np.array([[1,2,3],[3,2,1]], dtype=np.int), axis=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "标注病例中1658899这个病例放到无标注数据集上，因为这个病例的SC特别少。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
