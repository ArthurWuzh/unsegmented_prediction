{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTM 半监督模型的训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--root_path ROOT_PATH] [--exp EXP]\n",
      "                             [--max_iterations MAX_ITERATIONS]\n",
      "                             [--batch_size BATCH_SIZE]\n",
      "                             [--labeled_bs LABELED_BS] [--base_lr BASE_LR]\n",
      "                             [--deterministic DETERMINISTIC] [--seed SEED]\n",
      "                             [--gpu GPU] [--ema_decay EMA_DECAY]\n",
      "                             [--consistency_type CONSISTENCY_TYPE]\n",
      "                             [--consistency CONSISTENCY]\n",
      "                             [--consistency_rampup CONSISTENCY_RAMPUP]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-29231fc2-cb9c-4049-a74a-14756509a640.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py:3334: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "from tensorboardX import SummaryWriter\n",
    "import shutil\n",
    "import argparse\n",
    "import logging\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "from networks.vnet import VNet\n",
    "from dataloaders import utils\n",
    "from utils import ramps, losses\n",
    "from dataloaders.la_heart import LAHeart, RandomCrop, CenterCrop, RandomRotFlip, ToTensor, TwoStreamBatchSampler\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root_path', type=str, default='../data/2018LA_Seg_Training Set/', help='Name of Experiment')\n",
    "parser.add_argument('--exp', type=str,  default='UAMT_unlabel', help='model_name')\n",
    "parser.add_argument('--max_iterations', type=int,  default=6000, help='maximum epoch number to train')\n",
    "parser.add_argument('--batch_size', type=int, default=4, help='batch_size per gpu')\n",
    "parser.add_argument('--labeled_bs', type=int, default=2, help='labeled_batch_size per gpu')\n",
    "parser.add_argument('--base_lr', type=float,  default=0.01, help='maximum epoch number to train')\n",
    "parser.add_argument('--deterministic', type=int,  default=1, help='whether use deterministic training')\n",
    "parser.add_argument('--seed', type=int,  default=1337, help='random seed')\n",
    "parser.add_argument('--gpu', type=str,  default='0', help='GPU to use')\n",
    "### costs\n",
    "parser.add_argument('--ema_decay', type=float,  default=0.99, help='ema_decay')\n",
    "parser.add_argument('--consistency_type', type=str,  default=\"mse\", help='consistency_type')\n",
    "parser.add_argument('--consistency', type=float,  default=0.1, help='consistency')\n",
    "parser.add_argument('--consistency_rampup', type=float,  default=40.0, help='consistency_rampup')\n",
    "args = parser.parse_args()\n",
    "\n",
    "train_data_path = args.root_path\n",
    "snapshot_path = \"../model/\" + args.exp + \"/\"\n",
    "\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu\n",
    "batch_size = args.batch_size * len(args.gpu.split(','))\n",
    "max_iterations = args.max_iterations\n",
    "base_lr = args.base_lr\n",
    "labeled_bs = args.labeled_bs\n",
    "\n",
    "if args.deterministic:\n",
    "    cudnn.benchmark = False\n",
    "    cudnn.deterministic = True\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "num_classes = 2\n",
    "patch_size = (112, 112, 80)\n",
    "\n",
    "\n",
    "def get_current_consistency_weight(epoch):\n",
    "    # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "    return args.consistency * ramps.sigmoid_rampup(epoch, args.consistency_rampup)\n",
    "\n",
    "def update_ema_variables(model, ema_model, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(ema_model.parameters(), model.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ## make logger file\n",
    "    if not os.path.exists(snapshot_path):\n",
    "        os.makedirs(snapshot_path)\n",
    "    if os.path.exists(snapshot_path + '/code'):\n",
    "        shutil.rmtree(snapshot_path + '/code')\n",
    "    shutil.copytree('.', snapshot_path + '/code', shutil.ignore_patterns(['.git','__pycache__']))\n",
    "\n",
    "    logging.basicConfig(filename=snapshot_path+\"/log.txt\", level=logging.INFO,\n",
    "                        format='[%(asctime)s.%(msecs)03d] %(message)s', datefmt='%H:%M:%S')\n",
    "    logging.getLogger().addHandler(logging.StreamHandler(sys.stdout))\n",
    "    logging.info(str(args))\n",
    "\n",
    "    def create_model(ema=False):\n",
    "        # Network definition\n",
    "        net = VNet(n_channels=1, n_classes=num_classes, normalization='batchnorm', has_dropout=True)\n",
    "        model = net.cuda()\n",
    "        if ema:\n",
    "            for param in model.parameters():\n",
    "                param.detach_()\n",
    "        return model\n",
    "\n",
    "    model = create_model()\n",
    "    ema_model = create_model(ema=True)\n",
    "\n",
    "    db_train = LAHeart(base_dir=train_data_path,\n",
    "                       split='train',\n",
    "                       transform = transforms.Compose([\n",
    "                          RandomRotFlip(),\n",
    "                          RandomCrop(patch_size),\n",
    "                          ToTensor(),\n",
    "                          ]))\n",
    "    db_test = LAHeart(base_dir=train_data_path,\n",
    "                       split='test',\n",
    "                       transform = transforms.Compose([\n",
    "                           CenterCrop(patch_size),\n",
    "                           ToTensor()\n",
    "                       ]))\n",
    "    labeled_idxs = list(range(16))\n",
    "    unlabeled_idxs = list(range(16, 80))\n",
    "    batch_sampler = TwoStreamBatchSampler(labeled_idxs, unlabeled_idxs, batch_size, batch_size-labeled_bs)\n",
    "    def worker_init_fn(worker_id):\n",
    "        random.seed(args.seed+worker_id)\n",
    "    trainloader = DataLoader(db_train, batch_sampler=batch_sampler, num_workers=4, pin_memory=True, worker_init_fn=worker_init_fn)\n",
    "\n",
    "    model.train()\n",
    "    ema_model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=base_lr, momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "    if args.consistency_type == 'mse':\n",
    "        consistency_criterion = losses.softmax_mse_loss\n",
    "    elif args.consistency_type == 'kl':\n",
    "        consistency_criterion = losses.softmax_kl_loss\n",
    "    else:\n",
    "        assert False, args.consistency_type\n",
    "\n",
    "    writer = SummaryWriter(snapshot_path+'/log')\n",
    "    logging.info(\"{} itertations per epoch\".format(len(trainloader)))\n",
    "\n",
    "    iter_num = 0\n",
    "    max_epoch = max_iterations//len(trainloader)+1\n",
    "    lr_ = base_lr\n",
    "    model.train()\n",
    "    for epoch_num in tqdm(range(max_epoch), ncols=70):\n",
    "        time1 = time.time()\n",
    "        for i_batch, sampled_batch in enumerate(trainloader):\n",
    "            time2 = time.time()\n",
    "            # print('fetch data cost {}'.format(time2-time1))\n",
    "            volume_batch, label_batch = sampled_batch['image'], sampled_batch['label']\n",
    "            volume_batch, label_batch = volume_batch.cuda(), label_batch.cuda()\n",
    "            unlabeled_volume_batch = volume_batch[labeled_bs:]\n",
    "\n",
    "            noise = torch.clamp(torch.randn_like(unlabeled_volume_batch) * 0.1, -0.2, 0.2)\n",
    "            ema_inputs = unlabeled_volume_batch + noise\n",
    "            outputs = model(volume_batch)\n",
    "            with torch.no_grad():\n",
    "                ema_output = ema_model(ema_inputs)\n",
    "            T = 8\n",
    "            volume_batch_r = unlabeled_volume_batch.repeat(2, 1, 1, 1, 1)\n",
    "            stride = volume_batch_r.shape[0] // 2\n",
    "            preds = torch.zeros([stride * T, 2, 112, 112, 80]).cuda()\n",
    "            for i in range(T//2):\n",
    "                ema_inputs = volume_batch_r + torch.clamp(torch.randn_like(volume_batch_r) * 0.1, -0.2, 0.2)\n",
    "                with torch.no_grad():\n",
    "                    preds[2 * stride * i:2 * stride * (i + 1)] = ema_model(ema_inputs)\n",
    "            preds = F.softmax(preds, dim=1)\n",
    "            preds = preds.reshape(T, stride, 2, 112, 112, 80)\n",
    "            preds = torch.mean(preds, dim=0)  #(batch, 2, 112,112,80)\n",
    "            uncertainty = -1.0*torch.sum(preds*torch.log(preds + 1e-6), dim=1, keepdim=True) #(batch, 1, 112,112,80)\n",
    "\n",
    "\n",
    "            ## calculate the loss\n",
    "            loss_seg = F.cross_entropy(outputs[:labeled_bs], label_batch[:labeled_bs])\n",
    "            outputs_soft = F.softmax(outputs, dim=1)\n",
    "            loss_seg_dice = losses.dice_loss(outputs_soft[:labeled_bs, 1, :, :, :], label_batch[:labeled_bs] == 1)\n",
    "            supervised_loss = 0.5*(loss_seg+loss_seg_dice)\n",
    "\n",
    "            consistency_weight = get_current_consistency_weight(iter_num//150)\n",
    "            consistency_dist = consistency_criterion(outputs[labeled_bs:], ema_output) #(batch, 2, 112,112,80)\n",
    "            threshold = (0.75+0.25*ramps.sigmoid_rampup(iter_num, max_iterations))*np.log(2)\n",
    "            mask = (uncertainty<threshold).float()\n",
    "            consistency_dist = torch.sum(mask*consistency_dist)/(2*torch.sum(mask)+1e-16)\n",
    "            consistency_loss = consistency_weight * consistency_dist\n",
    "            loss = supervised_loss + consistency_loss\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            update_ema_variables(model, ema_model, args.ema_decay, iter_num)\n",
    "\n",
    "            iter_num = iter_num + 1\n",
    "            writer.add_scalar('uncertainty/mean', uncertainty[0,0].mean(), iter_num)\n",
    "            writer.add_scalar('uncertainty/max', uncertainty[0,0].max(), iter_num)\n",
    "            writer.add_scalar('uncertainty/min', uncertainty[0,0].min(), iter_num)\n",
    "            writer.add_scalar('uncertainty/mask_per', torch.sum(mask)/mask.numel(), iter_num)\n",
    "            writer.add_scalar('uncertainty/threshold', threshold, iter_num)\n",
    "            writer.add_scalar('lr', lr_, iter_num)\n",
    "            writer.add_scalar('loss/loss', loss, iter_num)\n",
    "            writer.add_scalar('loss/loss_seg', loss_seg, iter_num)\n",
    "            writer.add_scalar('loss/loss_seg_dice', loss_seg_dice, iter_num)\n",
    "            writer.add_scalar('train/consistency_loss', consistency_loss, iter_num)\n",
    "            writer.add_scalar('train/consistency_weight', consistency_weight, iter_num)\n",
    "            writer.add_scalar('train/consistency_dist', consistency_dist, iter_num)\n",
    "\n",
    "            logging.info('iteration %d : loss : %f cons_dist: %f, loss_weight: %f' %\n",
    "                         (iter_num, loss.item(), consistency_dist.item(), consistency_weight))\n",
    "            if iter_num % 50 == 0:\n",
    "                image = volume_batch[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('train/Image', grid_image, iter_num)\n",
    "\n",
    "                # image = outputs_soft[0, 3:4, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                image = torch.max(outputs_soft[0, :, :, :, 20:61:10], 0)[1].permute(2, 0, 1).data.cpu().numpy()\n",
    "                image = utils.decode_seg_map_sequence(image)\n",
    "                grid_image = make_grid(image, 5, normalize=False)\n",
    "                writer.add_image('train/Predicted_label', grid_image, iter_num)\n",
    "\n",
    "                image = label_batch[0, :, :, 20:61:10].permute(2, 0, 1)\n",
    "                grid_image = make_grid(utils.decode_seg_map_sequence(image.data.cpu().numpy()), 5, normalize=False)\n",
    "                writer.add_image('train/Groundtruth_label', grid_image, iter_num)\n",
    "\n",
    "                image = uncertainty[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('train/uncertainty', grid_image, iter_num)\n",
    "\n",
    "                mask2 = (uncertainty > threshold).float()\n",
    "                image = mask2[0, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('train/mask', grid_image, iter_num)\n",
    "                #####\n",
    "                image = volume_batch[-1, 0:1, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                grid_image = make_grid(image, 5, normalize=True)\n",
    "                writer.add_image('unlabel/Image', grid_image, iter_num)\n",
    "\n",
    "                # image = outputs_soft[-1, 3:4, :, :, 20:61:10].permute(3, 0, 1, 2).repeat(1, 3, 1, 1)\n",
    "                image = torch.max(outputs_soft[-1, :, :, :, 20:61:10], 0)[1].permute(2, 0, 1).data.cpu().numpy()\n",
    "                image = utils.decode_seg_map_sequence(image)\n",
    "                grid_image = make_grid(image, 5, normalize=False)\n",
    "                writer.add_image('unlabel/Predicted_label', grid_image, iter_num)\n",
    "\n",
    "                image = label_batch[-1, :, :, 20:61:10].permute(2, 0, 1)\n",
    "                grid_image = make_grid(utils.decode_seg_map_sequence(image.data.cpu().numpy()), 5, normalize=False)\n",
    "                writer.add_image('unlabel/Groundtruth_label', grid_image, iter_num)\n",
    "\n",
    "            ## change lr\n",
    "            if iter_num % 2500 == 0:\n",
    "                lr_ = base_lr * 0.1 ** (iter_num // 2500)\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_\n",
    "            if iter_num % 1000 == 0:\n",
    "                save_mode_path = os.path.join(snapshot_path, 'iter_' + str(iter_num) + '.pth')\n",
    "                torch.save(model.state_dict(), save_mode_path)\n",
    "                logging.info(\"save model to {}\".format(save_mode_path))\n",
    "\n",
    "            if iter_num >= max_iterations:\n",
    "                break\n",
    "            time1 = time.time()\n",
    "        if iter_num >= max_iterations:\n",
    "            break\n",
    "    save_mode_path = os.path.join(snapshot_path, 'iter_'+str(max_iterations)+'.pth')\n",
    "    torch.save(model.state_dict(), save_mode_path)\n",
    "    logging.info(\"save model to {}\".format(save_mode_path))\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'simpleITK'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-5dc69b000c6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msimpleITK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'simpleITK'"
     ]
    }
   ],
   "source": [
    "import simpleITK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
